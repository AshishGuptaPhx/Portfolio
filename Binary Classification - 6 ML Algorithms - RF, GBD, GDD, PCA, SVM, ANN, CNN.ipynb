{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Script Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading User-defined libraries developed by Author "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import sklearn           as sk\n",
    "import statistics        as st\n",
    "import pprint            as pp\n",
    "import math              as mth\n",
    "import seaborn           as sns\n",
    "import matplotlib        as mpl\n",
    "import os                as os\n",
    "import matplotlib.pyplot as plt\n",
    "import atexit\n",
    "import functools \n",
    "import importlib\n",
    "\n",
    "from time                  import clock\n",
    "\n",
    "from sklearn.datasets      import load_digits\n",
    "from sklearn.datasets      import fetch_olivetti_faces\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.linear_model  import LogisticRegressionCV\n",
    "from sklearn.linear_model  import Ridge\n",
    "from sklearn.linear_model  import Lasso\n",
    "from sklearn.linear_model  import SGDClassifier\n",
    "\n",
    "from sklearn.svm           import SVC\n",
    "\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    as LDA \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import cross_validate \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from sklearn.metrics         import mean_absolute_error\n",
    "from sklearn.metrics         import confusion_matrix\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from sklearn.metrics         import recall_score\n",
    "from sklearn.metrics         import precision_score\n",
    "from sklearn.metrics         import roc_curve\n",
    "from sklearn.metrics         import f1_score\n",
    "from sklearn.metrics         import roc_auc_score\n",
    "from sklearn.metrics         import classification_report\n",
    "\n",
    "from tensorflow                    import keras\n",
    "from tensorflow.keras.datasets     import cifar10\n",
    "from tensorflow.keras              import Sequential\n",
    "\n",
    "from tensorflow.keras.layers       import Dense\n",
    "from tensorflow.keras.layers       import Conv2D\n",
    "from tensorflow.keras.layers       import Flatten\n",
    "from tensorflow.keras.layers       import MaxPooling2D\n",
    "from tensorflow.keras.utils        import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading user-defined libraries for different pre-processing and model build functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "1:52:24.467 - Start Program\n",
      "========================================\n",
      "None\n",
      "Library COMMON loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'common' from 'C:\\\\Users\\\\HP\\\\Google Drive\\\\Notebooks\\\\Python\\\\Statistical Learning\\\\Innovations\\\\common.py'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Loading User-defined libraries developed by Author\n",
    "# ----------------------------------------------------------------------\n",
    "import timing\n",
    "import common\n",
    "importlib.reload(timing)\n",
    "importlib.reload(common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Original Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Fetching the Cifar Data and displaying the shape\n",
    "# ----------------------------------------------------------------------\n",
    "(X_CFR_train, y_CFR_train), (X_CFR_test, y_CFR_test) = cifar10.load_data()\n",
    "X_CFR_train.shape, y_CFR_train.shape, X_CFR_test.shape, y_CFR_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating the Subsets including dogs and horses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Retaining the images of dogs and horses only in Training and Test DS\n",
    "# ----------------------------------------------------------------------\n",
    "img_train_index = np.where((y_CFR_train == 5) | (y_CFR_train == 7))[0]\n",
    "img_test_index  = np.where((y_CFR_test  == 5) | (y_CFR_test  == 7))[0]\n",
    "\n",
    "X_CFR_train = X_CFR_train[img_train_index]\n",
    "y_CFR_train = y_CFR_train[img_train_index]\n",
    "\n",
    "X_CFR_test  = X_CFR_test [img_test_index]\n",
    "y_CFR_test  = y_CFR_test [img_test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Replacing Labels in Training and Test with boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Replacing the response values in the Displaying the final shape\n",
    "# ----------------------------------------------------------------------\n",
    "y_CFR_train = np.where(y_CFR_train == 7, 1, 0) \n",
    "y_CFR_test  = np.where(y_CFR_test == 7, 1, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Splitting the Data Subsets into training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 32, 32, 3),\n",
       " (8000, 1),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 1),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Splitting the X_Train and Y_Train by 80/20 split to create validation dataset\n",
    "# -----------------------------------------------------------------------------------\n",
    "X_CFR_train, X_CFR_valid, y_CFR_train, y_CFR_valid = train_test_split(X_CFR_train, \n",
    "                                                                      y_CFR_train, \n",
    "                                                                      test_size    = .2,  \n",
    "                                                                      stratify     = y_CFR_train, \n",
    "                                                                      random_state = 1)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Displaying the final shape\n",
    "# ----------------------------------------------------------------------\n",
    "X_CFR_train.shape, y_CFR_train.shape, X_CFR_valid.shape, y_CFR_valid.shape, X_CFR_test.shape, y_CFR_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Pre-processing for model build exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Refreshing the data-sets from master data subset for dogs and horses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 32, 32, 3),\n",
       " (8000, 1),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 1),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 1))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Reloading the Training and Test Data sets from the master dataset\n",
    "# --------------------------------------------------------------------\n",
    "X_train = X_CFR_train\n",
    "y_train = y_CFR_train\n",
    "\n",
    "X_valid = X_CFR_valid\n",
    "y_valid = y_CFR_valid\n",
    "\n",
    "X_test  = X_CFR_test\n",
    "y_test  = y_CFR_test\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Showing the current shapes\n",
    "# --------------------------------------------------------------------\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Flattening the feature training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 3072), (8000,), (2000, 3072), (2000,), (2000, 3072), (2000,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# New Modular Code for doing Random Forest Classification for images\n",
    "#------------------------------------------------------------------------\n",
    "X_train = common.f_flatten_img_ds(X_train)\n",
    "X_valid = common.f_flatten_img_ds(X_valid)\n",
    "X_test  = common.f_flatten_img_ds(X_test)\n",
    "\n",
    "y_train = y_train.reshape(8000)\n",
    "y_valid = y_valid.reshape(2000)\n",
    "y_test  = y_test.reshape(2000)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Showing the current shapes\n",
    "# --------------------------------------------------------------------\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Function for building PCA Components for all X training, validation and \n",
    "# test datasets\n",
    "#--------------------------------------------------------------------------------\n",
    "def f_build_PC_model(arg_X_train, arg_X_valid, arg_X_test, arg_DR_type, arg_exp_var):\n",
    "\n",
    "    #Build a default PCA model\n",
    "    if arg_DR_type == 'PCA':\n",
    "        dr_model = PCA()\n",
    "    elif arg_DR_type == 'NMF':\n",
    "        dr_model = NMF()\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "        \n",
    "    dr_model.fit_transform(arg_X_train)\n",
    "\n",
    "    # Calculating optimal k to have x% (say) variance \n",
    "    k = 0\n",
    "    total = sum(dr_model.explained_variance_)\n",
    "    current_sum = 0\n",
    "\n",
    "    while(current_sum / total < arg_exp_var):\n",
    "        current_sum += dr_model.explained_variance_[k]\n",
    "        k += 1\n",
    "\n",
    "    ## Applying PCA with k calculated above\n",
    "    dr_model2 = PCA(n_components = k, whiten = True)\n",
    "\n",
    "    X_train_pca = dr_model2.fit_transform(arg_X_train)\n",
    "    X_valid_pca = dr_model2.transform(arg_X_valid)\n",
    "    X_test_pca  = dr_model2.transform(arg_X_test)\n",
    "    \n",
    "    return (X_train_pca, X_valid_pca, X_test_pca, k)\n",
    "# --------------------- END OF FUNCTION --------------------------\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Dimensional Reduction using PCA with 95% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Components =  598\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Creating the PC Factors with 95% variance using PCA\n",
    "#------------------------------------------------------------------------\n",
    "X_train_PCA, X_valid_PCA, X_test_PCA, PCA_Factors = f_build_PC_model(X_train, X_valid, X_test, 'PCA', 0.99)\n",
    "print('PC Components = ', PCA_Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Dimensional Reduction using NMF with 95% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Creating the PC Factors with 95% variance using NMF\n",
    "#------------------------------------------------------------------------\n",
    "#X_train_NMF, X_valid_NMF, X_test_NMF, NMF_Factors = f_build_PC_model(X_train, X_valid, X_test, 'NMF', 0.95)\n",
    "#print('NMF Components = ', NMF_Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Function for generating Decision Tree based Model\n",
    "#------------------------------------------------------------------------\n",
    "def f_build_RF_CV(arg_X_train, arg_y_train, \n",
    "                  arg_X_valid, arg_y_valid, \n",
    "                  arg_X_test, arg_y_test,\n",
    "                  arg_model_type, arg_random_state, arg_fold):\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Initiating the Random Forest Model\n",
    "    #------------------------------------------------------------------------\n",
    "    print('stage 0 completed.')\n",
    "\n",
    "    if arg_model_type == 'DecisionTree':\n",
    "        None\n",
    "    elif arg_model_type == 'RF':\n",
    "        model = RandomForestClassifier(random_state = arg_random_state)\n",
    "    elif arg_model_type == 'GBD':\n",
    "        model = GradientBoostingClassifier()\n",
    "    elif arg_model_type == 'SGD':\n",
    "        model = SGDClassifier(random_state = arg_random_state)\n",
    "    else:\n",
    "        None\n",
    "    print('stage 1 completed.')\n",
    "    #------------------------------------------------------------------------\n",
    "    # Fitting the Decision Tree Type Model\n",
    "    #------------------------------------------------------------------------\n",
    "    model.fit(arg_X_train, arg_y_train)    \n",
    "    print('stage 2 completed.')\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Doing the Cross-validation on Valdation dataset\n",
    "    #------------------------------------------------------------------------\n",
    "    cv_val = cross_validate(model, arg_X_valid, arg_y_valid, cv = arg_fold)\n",
    "    print('stage 3 completed.')\n",
    "   \n",
    "    #------------------------------------------------------------------------\n",
    "    # Doing Predictions on Test dataset\n",
    "    #------------------------------------------------------------------------\n",
    "    y_test_pred = cross_val_predict(model, arg_X_test, arg_y_test, \n",
    "                                    cv = arg_fold, method = \"predict\")\n",
    "    print('stage 4 completed.')\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Creating a dictionary to store model metrics\n",
    "    #------------------------------------------------------------------------\n",
    "    model_metrics = {}\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Collecting metrics\n",
    "    #------------------------------------------------------------------------\n",
    "    model_accuracy      = accuracy_score (arg_y_test, y_test_pred)\n",
    "    #model_recall        = recall_score   (arg_y_test, y_test_pred)\n",
    "    #model_precision     = precision_score(arg_y_test, y_test_pred)\n",
    "    #model_roc_score     = roc_curve      (arg_y_test, y_test_pred)\n",
    "    #model_f1_score      = f1_score       (arg_y_test, y_test_pred)\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Storing metrics in the dictionary\n",
    "    #------------------------------------------------------------------------\n",
    "    model_metrics['accuracy_score']  = model_accuracy\n",
    "    #model_metrics['recall_score']    = model_recall\n",
    "    #model_metrics['precision_score'] = model_precision\n",
    "    #model_metrics['roc_score']       = model_roc_score\n",
    "    #model_metrics['f1_score']        = model_f1_score\n",
    "\n",
    "    print('Ready to return.')\n",
    "    return model_metrics\n",
    "# --------------------- END OF FUNCTION --------------------------\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 RandomForest 5-fold cross-validation based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 3072), (8000, 1), (2000, 3072), (2000, 1), (2000, 3072), (2000, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Displaying the final shape\n",
    "# ----------------------------------------------------------------------\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()\n",
    "y_valid = y_valid.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 0 completed.\n",
      "stage 1 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:28: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 2 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 3 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\HP\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 4 completed.\n",
      "Ready to return.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.7555}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Random Forest Model with 5 fold cross-validation\n",
    "#------------------------------------------------------------------------\n",
    "#from common import f_build_RF_CV\n",
    "\n",
    "model1 = f_build_RF_CV(X_train, y_train, X_valid, y_valid, X_test, y_test,\n",
    "                              'RF', 42, 5)\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.2 Gradient Boosting Decision Trees 5-fold cross-validation based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Gradient Boosting Decision Tree Model with 5 fold cross-validation\n",
    "#--------------------------------------------------------------------------------\n",
    "model2 = common.f_build_RF_CV(X_train, y_train, X_valid, y_valid, X_test, y_test,\n",
    "                              'GBD', 42, 5)\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Descent Decision Trees 5-fold cross-validation based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Stoichastic Gradient Descent Decision Tree Model with 5 fold cross-validation\n",
    "#--------------------------------------------------------------------------------\n",
    "model3 = common.f_build_RF_CV(X_train, y_train, X_valid, y_valid, X_test, y_test,\n",
    "                              'SGD', 42, 5)\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 RandomForest 5-fold cross-validation based model using PCA factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Random Forest Model with 5 fold cross-validation\n",
    "#------------------------------------------------------------------------\n",
    "model4 = f_build_RF_CV(X_train_PCA, y_train, X_valid_PCA, y_valid, X_test_PCA, y_test,\n",
    "                              'RF', 42, 5)\n",
    "model4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.5 Gradient Boosting Decision Trees 5-fold cross-validation based model using PCA factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = common.f_build_RF_CV(X_train_PCA, y_train, X_valid_PCA, y_valid, X_test_PCA, y_test,\n",
    "                              'GBD', 42, 5)\n",
    "model5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Gradient Descent Decision Trees 5-fold cross-validation based model using PCA factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Stoichastic Gradient Descent Decision Tree Model with 5 fold cross-validation\n",
    "#--------------------------------------------------------------------------------\n",
    "model6 = common.f_build_RF_CV(X_train_PCA, y_train, X_valid_PCA, y_valid, X_test_PCA, y_test,\n",
    "                              'SGD', 42, 5)\n",
    "model6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Support Vector based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Function for building PCA Components for all X training, validation and \n",
    "# test datasets\n",
    "#--------------------------------------------------------------------------------\n",
    "def f_build_SVM_model(arg_X_train, arg_y_train, arg_X_valid, arg_y_valid, arg_X_test, arg_y_test,\n",
    "                      arg_model_type, arg_kernel, arg_gamma, arg_C_vals, arg_cv, arg_metric):\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Running a loop over the model dictionary\n",
    "    # -----------------------------------------------------------------------\n",
    "    print('MODEL: ', arg_model_type)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Applying GridSearch approach only for SVC and SVM\n",
    "    # -----------------------------------------------------------------------\n",
    "    if arg_model_type == 'SVC': \n",
    "        mdl = SVC(kernel = arg_kernel, probability = True)\n",
    "        gs = GridSearchCV(mdl, param_grid = {'C':arg_C_vals}, cv = arg_cv, scoring = arg_metric).fit(X_train, y_train)    \n",
    "        print('For Model ', arg_model_type,  ', Best C Value is : ', gs.best_params_)\n",
    "    elif arg_model_type == 'SVM': \n",
    "        mdl = SVC(kernel = arg_kernel, gamma = arg_gamma, probability = True).fit(X_train, y_train)\n",
    "        gs = GridSearchCV(mdl, param_grid = {'C':arg_C_vals}, cv = arg_cv, scoring = arg_metric).fit(X_train, y_train)    \n",
    "        print('For Model ', arg_model_type,  ', Best C Value is : ', gs.best_params_)\n",
    "    elif arg_model_type == 'LDA': \n",
    "        mdl = LDA()\n",
    "        gs = mdl.fit(arg_X_train, arg_y_train)\n",
    "    elif arg_model_type == 'QDA': \n",
    "        mdl = QDA()\n",
    "        gs = mdl.fit(arg_X_train, arg_y_train)\n",
    "    else:\n",
    "        None    \n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Doing Training and Test Data Set Predictions\n",
    "    # -----------------------------------------------------------------------\n",
    "    #preds_train_ds = gs.predict(X_train)\n",
    "    #probs_train_ds = gs.predict_proba(X_train)\n",
    "    y_test_pred  = gs.predict(arg_X_test)\n",
    "    y_test_probs = gs.predict_proba(arg_X_test)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Creating a dictionary to store model metrics\n",
    "    #------------------------------------------------------------------------\n",
    "    model_metrics = {}\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Collecting metrics\n",
    "    #------------------------------------------------------------------------\n",
    "    model_accuracy      = accuracy_score (arg_y_test, y_test_pred)\n",
    "    model_recall        = recall_score   (arg_y_test, y_test_pred)\n",
    "    model_precision     = precision_score(arg_y_test, y_test_pred)\n",
    "    model_roc_score     = roc_curve      (arg_y_test, y_test_pred)\n",
    "    model_f1_score      = f1_score       (arg_y_test, y_test_pred)\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Storing metrics in the dictionary\n",
    "    #------------------------------------------------------------------------\n",
    "    model_metrics['accuracy_score']  = model_accuracy\n",
    "    model_metrics['recall_score']    = model_recall\n",
    "    model_metrics['precision_score'] = model_precision\n",
    "    model_metrics['roc_score']       = model_roc_score\n",
    "    model_metrics['f1_score']        = model_f1_score\n",
    "    \n",
    "    return model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Setting up the range of tuning parameter from 0.1 and 10\n",
    "# -----------------------------------------------------------------------\n",
    "c_vals = list(np.arange(0.1, 2.1, 1.0))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Setting up Model Dictionary for SVC, SVM, LDA and QDA Models\n",
    "# -----------------------------------------------------------------------\n",
    "model7 = f_build_SVM_model(X_train, y_train, X_valid, y_valid, X_test, y_test,\n",
    "                      'SVM', 'rbf', 'scale', c_vals, 4, 'accuracy')\n",
    "\n",
    "model7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Feed-forward Neural Network and Convolutional Neural Network based Image Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Reloading the Training and Test Data sets from the master dataset\n",
    "# --------------------------------------------------------------------\n",
    "X_train = X_CFR_train\n",
    "y_train = y_CFR_train\n",
    "X_valid = X_CFR_valid\n",
    "y_valid = y_CFR_valid\n",
    "X_test  = X_CFR_test\n",
    "y_test  = y_CFR_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 32, 32, 3),\n",
       " (8000, 1),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 1),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 1))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Displaying the shapes\n",
    "# --------------------------------------------------------------------\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Y variable before transformation: [1]\n",
      "Example Y variable after transformation: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Transforming training and test response values\n",
    "# --------------------------------------------------------------------\n",
    "print('Example Y variable before transformation:', y_train[0])\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('Example Y variable after transformation:', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 32, 32, 3),\n",
       " (8000, 2),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 2),\n",
       " (2000, 32, 32, 3),\n",
       " (2000, 2))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Displaying the shapes\n",
    "# --------------------------------------------------------------------\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library COMMON loaded.\n",
      "Current CONV Item :  [10, (3, 3), 1, 'relu', 'same', (32, 32, 3)]\n",
      "Current POOL Item :  [(2, 2), 2]\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 10)        280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 5122      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 5,408\n",
      "Trainable params: 5,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 4s 471us/sample - loss: 0.9635 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 1s 182us/sample - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 1s 177us/sample - loss: 0.6932 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 1s 184us/sample - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 1s 186us/sample - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 2s 188us/sample - loss: 0.6932 - accuracy: 0.4930 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 1s 179us/sample - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 1s 179us/sample - loss: 0.6932 - accuracy: 0.4938 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 1s 175us/sample - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 1s 182us/sample - loss: 0.6933 - accuracy: 0.4942 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 1s 177us/sample - loss: 0.6932 - accuracy: 0.4965 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 1s 177us/sample - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 1s 182us/sample - loss: 0.6932 - accuracy: 0.4880 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 1s 187us/sample - loss: 0.6933 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 1s 186us/sample - loss: 0.6932 - accuracy: 0.4922 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 1s 186us/sample - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 1s 184us/sample - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 1s 184us/sample - loss: 0.6932 - accuracy: 0.4897 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 1s 185us/sample - loss: 0.6932 - accuracy: 0.4975 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 1s 184us/sample - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 1s 184us/sample - loss: 0.6932 - accuracy: 0.4915 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 2s 188us/sample - loss: 0.6932 - accuracy: 0.4938 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 1s 186us/sample - loss: 0.6932 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 1s 185us/sample - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 1s 182us/sample - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 1s 184us/sample - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 2s 190us/sample - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 2s 194us/sample - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 2s 193us/sample - loss: 0.6932 - accuracy: 0.4978 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 2s 192us/sample - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Model Accuracy is  0.5\n"
     ]
    }
   ],
   "source": [
    "from common import f_build_CNN\n",
    "importlib.reload(common)\n",
    "\n",
    "arg_conv_layers = [[32, (3,3), 1, 'relu', 'same', (32,32,3)],\n",
    "                   [20, (3,3), 1, 'relu', 'same', (32,32,3)]]\n",
    "\n",
    "arg_conv_layers = [[10, (3,3), 1, 'relu', 'same', (32,32,3)]]\n",
    "arg_pool_layers = [[(2,2), 2]]\n",
    "arg_out_layers = [[2, 'relu'], [2, 'softmax']]\n",
    "arg_compile_parms = [['adam', 'categorical_crossentropy', ['accuracy']]] \n",
    "arg_fit_parms = [[30, 30]]\n",
    "\n",
    "model1 = f_build_CNN(X_train, y_train, X_test, y_test, \n",
    "                arg_conv_layers, \n",
    "                arg_pool_layers, \n",
    "                arg_out_layers,\n",
    "                arg_compile_parms,\n",
    "                arg_fit_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_CNN_feature_maps('CNN', X_train, y_train, X_test, y_test, 10):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32, 32, 8)         32        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32, 32, 16)        144       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32, 32, 32)        544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32, 32, 64)        2112      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 131074    \n",
      "=================================================================\n",
      "Total params: 133,906\n",
      "Trainable params: 133,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 10s 1ms/sample - loss: 1.9380 - accuracy: 0.7178 - val_loss: 0.5047 - val_accuracy: 0.7465\n",
      "Epoch 2/20\n",
      "8000/8000 [==============================] - 7s 865us/sample - loss: 0.4728 - accuracy: 0.7826 - val_loss: 0.5017 - val_accuracy: 0.7565\n",
      "Epoch 3/20\n",
      "8000/8000 [==============================] - 7s 865us/sample - loss: 0.4422 - accuracy: 0.7955 - val_loss: 0.4924 - val_accuracy: 0.7690\n",
      "Epoch 4/20\n",
      "8000/8000 [==============================] - 7s 866us/sample - loss: 0.4301 - accuracy: 0.8086 - val_loss: 0.5094 - val_accuracy: 0.7705\n",
      "Epoch 5/20\n",
      "8000/8000 [==============================] - 7s 866us/sample - loss: 0.4147 - accuracy: 0.8126 - val_loss: 0.5879 - val_accuracy: 0.7255\n",
      "Epoch 6/20\n",
      "8000/8000 [==============================] - 7s 865us/sample - loss: 0.3913 - accuracy: 0.8291 - val_loss: 0.6445 - val_accuracy: 0.7615\n",
      "Epoch 7/20\n",
      "8000/8000 [==============================] - 7s 867us/sample - loss: 0.3714 - accuracy: 0.8376 - val_loss: 0.5818 - val_accuracy: 0.7590\n",
      "Epoch 8/20\n",
      "8000/8000 [==============================] - 8s 944us/sample - loss: 0.3618 - accuracy: 0.8443 - val_loss: 0.5397 - val_accuracy: 0.7725\n",
      "Epoch 9/20\n",
      "8000/8000 [==============================] - 7s 868us/sample - loss: 0.3463 - accuracy: 0.8455 - val_loss: 0.5510 - val_accuracy: 0.7665\n",
      "Epoch 10/20\n",
      "8000/8000 [==============================] - 8s 951us/sample - loss: 0.3278 - accuracy: 0.8597 - val_loss: 0.5766 - val_accuracy: 0.7735\n",
      "Epoch 11/20\n",
      "8000/8000 [==============================] - 7s 869us/sample - loss: 0.3102 - accuracy: 0.8645 - val_loss: 0.6550 - val_accuracy: 0.7425\n",
      "Epoch 12/20\n",
      "8000/8000 [==============================] - 7s 866us/sample - loss: 0.3222 - accuracy: 0.8615 - val_loss: 0.6769 - val_accuracy: 0.7320\n",
      "Epoch 13/20\n",
      "8000/8000 [==============================] - 7s 932us/sample - loss: 0.2685 - accuracy: 0.8875 - val_loss: 0.6636 - val_accuracy: 0.7530\n",
      "Epoch 14/20\n",
      "8000/8000 [==============================] - 9s 1ms/sample - loss: 0.2673 - accuracy: 0.8930 - val_loss: 0.6864 - val_accuracy: 0.7460\n",
      "Epoch 15/20\n",
      "8000/8000 [==============================] - 8s 950us/sample - loss: 0.2410 - accuracy: 0.9028 - val_loss: 0.7656 - val_accuracy: 0.7465\n",
      "Epoch 16/20\n",
      "8000/8000 [==============================] - 7s 869us/sample - loss: 0.2113 - accuracy: 0.9130 - val_loss: 0.7740 - val_accuracy: 0.7535\n",
      "Epoch 17/20\n",
      "8000/8000 [==============================] - 7s 870us/sample - loss: 0.2178 - accuracy: 0.9174 - val_loss: 0.8143 - val_accuracy: 0.7490\n",
      "Epoch 18/20\n",
      "8000/8000 [==============================] - 7s 870us/sample - loss: 0.1851 - accuracy: 0.9244 - val_loss: 0.8288 - val_accuracy: 0.7130\n",
      "Epoch 19/20\n",
      "8000/8000 [==============================] - 7s 869us/sample - loss: 0.1831 - accuracy: 0.9245 - val_loss: 0.9700 - val_accuracy: 0.7435\n",
      "Epoch 20/20\n",
      "8000/8000 [==============================] - 7s 908us/sample - loss: 0.1828 - accuracy: 0.9295 - val_loss: 1.0423 - val_accuracy: 0.7470\n",
      "Model Accuracy is  0.747\n"
     ]
    }
   ],
   "source": [
    "from common import f_build_ANN\n",
    "\n",
    "arg_in_layers     = [[8, 'relu', (32, 32, 3)], \n",
    "                     [16, 'relu', (32, 32, 3)], \n",
    "                     [32, 'relu', (32, 32, 3)], \n",
    "                     [64, 'relu', (32, 32, 3)]]\n",
    "arg_out_layers    = [[2, 'softmax']]\n",
    "arg_compile_parms = [['adam', 'categorical_crossentropy', ['accuracy']]] \n",
    "arg_fit_parms = [[20, 20]]\n",
    "#arg_in_layers.reverse()\n",
    "#arg_in_layers\n",
    "model1 = f_build_ANN(X_train, y_train, X_test, y_test, \n",
    "                     arg_in_layers, arg_out_layers, arg_compile_parms, arg_fit_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
